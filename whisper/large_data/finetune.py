# save as: train_eln_prefix_lora.py
import os, json
from dataclasses import dataclass
from typing import List, Dict, Any
import torch
import torch.nn as nn
from torch.utils.data import Dataset
from transformers import (AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import PreTrainedModel


# --------------------------
# Configurable params
# --------------------------
MODEL_NAME = "meta-llama/Llama-2-7b-hf"   # gated; switch if needed
MAX_INPUT_TOKENS = 1024
PREFIX_TOKENS = 8
LR = 2e-4
EPOCHS = 3
OUTPUT_DIR = "./llama_eln_prefix_lora"
TARGET_MODULES = ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]

INSTR_HEADER = (
    "You are a transcription error correction assistant and linguistics expert, "
    "specializing in improving transcriptions produced by Automatic Speech Recognition (ASR) systems for noisy speeches. "
    "The levels of noise may vary widely. "
    "Your task is to perform error correction based on the words in top 5 hypotheses generated by the ASR system and the language-space noise summary. "
    "You can also correct the sentences yourself based on your understanding "
    "of their meaning and knowing the correct spelling of the words, but the meaning of the sentences should not be changed. "
    "Also, do not use synonyms, but only correct the spelling of the words.\n\n"
    "Analyze the linguistic context and provide the corrected ASR hypothesis directly without any explanations or additional commentary.\n\n"
)

def build_prompt(hypotheses: List[str]) -> str:
    s = INSTR_HEADER + "Hypotheses:\n"
    for i, h in enumerate(hypotheses, 1):
        s += f"<hypothesis{i}>{h}</hypothesis{i}>\n"
    s += "=>Correct Transcription:"
    return s

# --------------------------
# Dataset (PT version)
# --------------------------
class ASRPtDataset(Dataset):
    def __init__(self, path: str, tokenizer: AutoTokenizer):
        super().__init__()
        self.tokenizer = tokenizer
        self.records = torch.load(path)   # load list[dict]
        assert isinstance(self.records, list) and len(self.records) > 0, "Empty or invalid dataset!"

    def __len__(self):
        return len(self.records)

    def __getitem__(self, idx):
        r = self.records[idx]
        hyps: List[str] = r["hypotheses"]
        label: str = r["label"]
        eln: List[float] = r["eln"]

        prompt = build_prompt(hyps)
        prompt_ids = self.tokenizer(prompt, add_special_tokens=False)["input_ids"]
        target_ids = self.tokenizer(label + self.tokenizer.eos_token, add_special_tokens=False)["input_ids"]

        max_prompt = min(len(prompt_ids), MAX_INPUT_TOKENS - len(target_ids))
        prompt_ids = prompt_ids[:max_prompt]
        if len(prompt_ids) + len(target_ids) > MAX_INPUT_TOKENS:
            target_ids = target_ids[:MAX_INPUT_TOKENS - len(prompt_ids)]

        input_ids = prompt_ids + target_ids
        attention_mask = [1] * len(input_ids)
        labels = [-100] * len(prompt_ids) + target_ids

        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.long),
            "labels_text": torch.tensor(labels, dtype=torch.long),
            "eln": eln.clone().detach().to(dtype=torch.float32, device="cpu"),
            "prompt_len": len(prompt_ids),
            "text_len": len(input_ids),
        }



@dataclass
class DataCollatorPad:
    tokenizer: AutoTokenizer

    def __call__(self, batch):
        # infer max_len from input_ids length
        max_len = max(len(x["input_ids"]) for x in batch)

        def pad(seq, pad_val):
            out = []
            for x in batch:
                t = x[seq]
                pad_amt = max_len - t.shape[0]
                if pad_amt > 0:
                    t = torch.cat([t, torch.full((pad_amt,), pad_val, dtype=t.dtype)])
                out.append(t)
            return torch.stack(out, dim=0)

        input_ids   = pad("input_ids", pad_val=self.tokenizer.pad_token_id or self.tokenizer.eos_token_id)
        attention   = pad("attention_mask", pad_val=0)
        labels_text = pad("labels_text", pad_val=-100)
        eln_mat     = torch.stack([x["eln"] for x in batch], dim=0)

        return {
            "input_ids": input_ids,
            "attention_mask": attention,
            "labels_text": labels_text,
            "eln": eln_mat
        }


# --------------------------
# Model wrapper: ELN â†’ prefix embeddings
# --------------------------
class ModelWithELNPrefix(PreTrainedModel):
    config_class = None  # optional; can reuse base.config if needed

    def __init__(self, base_model: AutoModelForCausalLM, tokenizer, eln_dim: int, prefix_tokens: int):
        super().__init__(base_model.config)
        self.base = base_model
        self.tok = tokenizer
        self.prefix_tokens = prefix_tokens
        self.eln_dim = eln_dim
        hidden = self.base.config.hidden_size

        self.eln_proj = nn.Sequential(
            nn.Linear(eln_dim, hidden * prefix_tokens),
            nn.Tanh()
        )

    def forward(self, input_ids=None, attention_mask=None, labels_text=None, eln=None, **kwargs):
        # same forward logic as before ...
        text_embeds = self.base.get_input_embeddings()(input_ids)
        B = input_ids.shape[0]
        proj = self.eln_proj(eln)
        prefix_embeds = proj.view(B, self.prefix_tokens, -1)

        inputs_embeds = torch.cat([prefix_embeds, text_embeds], dim=1)
        prefix_mask = torch.ones(B, self.prefix_tokens, dtype=attention_mask.dtype, device=attention_mask.device)
        full_mask = torch.cat([prefix_mask, attention_mask], dim=1)

        prefix_ignore = torch.full((B, self.prefix_tokens), -100, dtype=labels_text.dtype, device=labels_text.device)
        full_labels = torch.cat([prefix_ignore, labels_text], dim=1)

        return self.base(
            inputs_embeds=inputs_embeds,
            attention_mask=full_mask,
            labels=full_labels
        )

    def gradient_checkpointing_enable(self, **kwargs):
        return self.base.gradient_checkpointing_enable(**kwargs)

    def gradient_checkpointing_disable(self):
        return self.base.gradient_checkpointing_disable()

    def save_pretrained(self, *args, **kwargs):
        self.base.save_pretrained(*args, **kwargs)

    @torch.no_grad()
    def safe_generate_with_eln(
        self, prompt_text: str, eln_vec, max_new_tokens=64, temperature=0.2, top_p=0.95
    ) -> str:
        """Safer wrapper for inference with ELN conditioning."""

        device = next(self.parameters()).device

        # --- 1. Check ELN vector shape ---
        eln_vec = torch.tensor(eln_vec, dtype=torch.float32, device=device)
        if eln_vec.shape[-1] != self.eln_dim:
            raise ValueError(f"ELN dim mismatch: got {eln_vec.shape[-1]}, expected {self.eln_dim}")

        # --- 2. Tokenize prompt ---
        enc = self.tok(prompt_text, return_tensors="pt", add_special_tokens=False).to(device)
        text_embeds = self.base.get_input_embeddings()(enc["input_ids"])
        B = text_embeds.shape[0]

        # --- 3. Project ELN into prefix embeddings ---
        proj = self.eln_proj(eln_vec.unsqueeze(0))                   # [1, K*H]
        prefix_embeds = proj.view(1, self.prefix_tokens, -1)         # [1, K, H]

        # --- 4. Concat prefix + text embeddings ---
        inputs_embeds = torch.cat([prefix_embeds, text_embeds], dim=1)
        full_mask = torch.cat(
            [torch.ones(B, self.prefix_tokens, device=device, dtype=enc["attention_mask"].dtype), enc["attention_mask"]],
            dim=1
        )

        # --- 5. Run generation ---
        gen_ids = self.base.generate(
            inputs_embeds=inputs_embeds,
            attention_mask=full_mask,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_p=top_p,
            eos_token_id=self.tok.eos_token_id,
            pad_token_id=self.tok.eos_token_id
        )

        # --- 6. Slice only the generated continuation ---
        # HF sometimes includes the whole prefix; sometimes only new tokens.
        gen_len = gen_ids.shape[1]
        prompt_len = enc["input_ids"].shape[1] + self.prefix_tokens
        if gen_len > prompt_len:
            continuation_ids = gen_ids[0][prompt_len:]
        else:
            continuation_ids = gen_ids[0]

        return self.tok.decode(continuation_ids, skip_special_tokens=True)

def load_base_model_and_tokenizer():
    tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
    tok.pad_token = tok.eos_token

    base = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        load_in_4bit=True,
        torch_dtype=torch.float16,
        device_map="auto",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
    )
    return base, tok

def wrap_with_lora(model):
    model = prepare_model_for_kbit_training(model)
    lora = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=TARGET_MODULES,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )
    model = get_peft_model(model, lora)
    return model

# --------------------------
# Training entry
# --------------------------
def main_train(train_path: str):
    # Infer ELN dim from first record
    first = torch.load(train_path)[0]
    eln_dim = len(first["eln"])

    base, tok = load_base_model_and_tokenizer()
    base = wrap_with_lora(base)
    model = ModelWithELNPrefix(base, tok, eln_dim=eln_dim, prefix_tokens=PREFIX_TOKENS)

    train_ds = ASRPtDataset(train_path, tok)
    collator = DataCollatorPad(tokenizer=tok)

    args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=EPOCHS,
        learning_rate=LR,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=16,
        fp16=True,
        bf16=False,
        lr_scheduler_type="cosine",
        warmup_steps=200,
        weight_decay=0.01,
        gradient_checkpointing=True,
        logging_steps=100,
        save_steps=1000,
        save_total_limit=2,
        report_to="none"
    )

    trainer = Trainer(
        model=model,
        args=args,
        data_collator=collator,
        tokenizer=tok,
        train_dataset=train_ds
    )

    trainer.train()

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    model.base.save_pretrained(OUTPUT_DIR)
    tok.save_pretrained(OUTPUT_DIR)
    torch.save(model.eln_proj.state_dict(), os.path.join(OUTPUT_DIR, "eln_proj.pt"))
    print("Saved to", OUTPUT_DIR)

if __name__ == "__main__":
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument("--train_pt", required=True)
    args = p.parse_args()
    main_train(args.train_pt)
