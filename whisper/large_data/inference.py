# eval_eln_prefix_lora.py
import os
from typing import List
import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
import editdistance
import torch.nn as nn
from transformers import PreTrainedModel

# --------------------------
# Instruction header
# --------------------------
INSTR_HEADER = (
    "You are a transcription error correction assistant and linguistics expert, "
    "specializing in improving transcriptions produced by Automatic Speech Recognition (ASR) systems for noisy speeches. "
    "The levels of noise may vary widely. "
    "Your task is to perform error correction based on the words in top 5 hypotheses generated by the ASR system and the language-space noise summary. "
    "You can also correct the sentences yourself based on your understanding "
    "of their meaning and knowing the correct spelling of the words, but the meaning of the sentences should not be changed. "
    "Also, do not use synonyms, but only correct the spelling of the words.\n\n"
    "Analyze the linguistic context and provide the corrected ASR hypothesis directly without any explanations or additional commentary.\n\n"
)

def build_prompt(hypotheses: List[str]) -> str:
    s = INSTR_HEADER + "Hypotheses:\n"
    for i, h in enumerate(hypotheses, 1):
        s += f"<hypothesis{i}>{h}</hypothesis{i}>\n"
    s += "=>Correct Transcription:"
    return s

# --------------------------
# Model wrapper
# --------------------------
class ModelWithELNPrefix(PreTrainedModel):
    config_class = None

    def __init__(self, base_model: AutoModelForCausalLM, tokenizer, eln_dim: int, prefix_tokens: int):
        super().__init__(base_model.config)
        self.base = base_model
        self.tok = tokenizer
        self.prefix_tokens = prefix_tokens
        self.eln_dim = eln_dim
        hidden = self.base.config.hidden_size

        self.eln_proj = nn.Sequential(
            nn.Linear(eln_dim, hidden * prefix_tokens),
            nn.Tanh()
        )

    def forward(self, input_ids=None, attention_mask=None, labels_text=None, eln=None, **kwargs):
        text_embeds = self.base.get_input_embeddings()(input_ids)
        B = input_ids.shape[0]

        # ensure ELN and eln_proj are on same device and dtype
        dtype = text_embeds.dtype
        device = text_embeds.device
        eln = eln.to(device=device, dtype=dtype)
        self.eln_proj = self.eln_proj.to(device=device, dtype=dtype)

        proj = self.eln_proj(eln)
        prefix_embeds = proj.view(B, self.prefix_tokens, -1)

        inputs_embeds = torch.cat([prefix_embeds, text_embeds], dim=1)
        prefix_mask = torch.ones(B, self.prefix_tokens, dtype=attention_mask.dtype, device=attention_mask.device)
        full_mask = torch.cat([prefix_mask, attention_mask], dim=1)

        prefix_ignore = torch.full((B, self.prefix_tokens), -100, dtype=labels_text.dtype, device=labels_text.device)
        full_labels = torch.cat([prefix_ignore, labels_text], dim=1)

        return self.base(
            inputs_embeds=inputs_embeds,
            attention_mask=full_mask,
            labels=full_labels
        )

    def gradient_checkpointing_enable(self, **kwargs):
        return self.base.gradient_checkpointing_enable(**kwargs)

    def gradient_checkpointing_disable(self):
        return self.base.gradient_checkpointing_disable()

    def save_pretrained(self, *args, **kwargs):
        self.base.save_pretrained(*args, **kwargs)

    @torch.no_grad()
    def safe_generate_with_eln(
        self, prompt_text: str, eln_vec, max_new_tokens=64, temperature=0.2, top_p=0.95
    ) -> str:
        """Generate text conditioned on ELN vector."""

        device = next(self.parameters()).device
        dtype = next(self.parameters()).dtype

        # move ELN vector to correct device & dtype
        eln_vec = eln_vec.clone().detach().to(device=device, dtype=dtype)

        # move ELN projection to correct device & dtype
        self.eln_proj = self.eln_proj.to(device=device, dtype=dtype)

        # --- tokenize prompt ---
        enc = self.tok(prompt_text, return_tensors="pt", add_special_tokens=False).to(device)
        text_embeds = self.base.get_input_embeddings()(enc["input_ids"])
        B = text_embeds.shape[0]

        # --- project ELN ---
        proj = self.eln_proj(eln_vec.unsqueeze(0))                   # [1, K*H]
        prefix_embeds = proj.view(1, self.prefix_tokens, -1)         # [1, K, H]

        # --- concat prefix + text embeddings ---
        inputs_embeds = torch.cat([prefix_embeds, text_embeds], dim=1)
        full_mask = torch.cat(
            [torch.ones(B, self.prefix_tokens, device=device, dtype=enc["attention_mask"].dtype), enc["attention_mask"]],
            dim=1
        )

        # --- generate ---
        gen_ids = self.base.generate(
            inputs_embeds=inputs_embeds,
            attention_mask=full_mask,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_p=top_p,
            eos_token_id=self.tok.eos_token_id,
            pad_token_id=self.tok.eos_token_id
        )

        # --- slice only the generated continuation ---
        gen_len = gen_ids.shape[1]
        prompt_len = enc["input_ids"].shape[1] + self.prefix_tokens
        continuation_ids = gen_ids[0][prompt_len:] if gen_len > prompt_len else gen_ids[0]

        return self.tok.decode(continuation_ids, skip_special_tokens=True)


# --------------------------
# Dataset
# --------------------------
class ASRPtDataset(Dataset):
    def __init__(self, path: str):
        self.records = torch.load(path)   # list of dicts
        assert isinstance(self.records, list) and len(self.records) > 0

    def __len__(self):
        return len(self.records)

    def __getitem__(self, idx):
        return self.records[idx]


# --------------------------
# WER utility
# --------------------------
def calculate_wer(pred: str, ref: str) -> float:
    return editdistance.eval(pred.split(), ref.split()) / max(1, len(ref.split()))


# --------------------------
# Load model
# --------------------------
def load_model(ckpt_dir: str, eln_dim: int, prefix_tokens: int):
    tok = AutoTokenizer.from_pretrained(ckpt_dir, use_fast=False)
    tok.pad_token = tok.eos_token

    base = AutoModelForCausalLM.from_pretrained(
        ckpt_dir,
        torch_dtype=torch.float16,
        device_map="auto"
    )

    model = ModelWithELNPrefix(base, tok, eln_dim=eln_dim, prefix_tokens=prefix_tokens)

    # load ELN projection if exists
    eln_proj_path = os.path.join(ckpt_dir, "eln_proj.pt")
    if os.path.exists(eln_proj_path):
        state = torch.load(eln_proj_path, map_location="cpu")
        model.eln_proj.load_state_dict(state)
        model.eln_proj.to(next(model.parameters()).device, dtype=next(model.parameters()).dtype)
    else:
        raise FileNotFoundError(f"Missing eln_proj.pt in {ckpt_dir}")

    model.eval()
    return model, tok


# --------------------------
# Evaluation loop
# --------------------------
def main_eval(test_pt: str, ckpt_dir: str, prefix_tokens: int = 8, save_out: str = None):
    first = torch.load(test_pt)[0]
    eln_dim = len(first["eln"])

    dataset = ASRPtDataset(test_pt)
    model, tok = load_model(ckpt_dir, eln_dim, prefix_tokens)

    device = next(model.parameters()).device
    dtype = next(model.parameters()).dtype

    preds, refs, wers = [], [], []

    for r in tqdm(dataset, desc="Evaluating"):
        hyps = r["hypotheses"]
        label = r["label"]
        eln = torch.tensor(r["eln"], dtype=dtype, device=device)

        prompt = build_prompt(hyps)
        pred = model.safe_generate_with_eln(prompt, eln)

        wer = calculate_wer(pred, label)
        preds.append(pred)
        refs.append(label)
        wers.append(wer)

    avg_wer = sum(wers) / len(wers)
    print(f"Average WER on test set: {avg_wer:.4f}")

    if save_out:
        out_path = save_out if save_out.endswith(".txt") else save_out + ".txt"
        with open(out_path, "w", encoding="utf-8") as f:
            for p, r, w in zip(preds, refs, wers):
                f.write(f"PRED: {p}\nREF: {r}\nWER: {w:.4f}\n\n")
        print("Saved detailed outputs to", out_path)


# --------------------------
# CLI
# --------------------------
if __name__ == "__main__":
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument("--test_pt", required=True)
    p.add_argument("--ckpt_dir", required=True)
    p.add_argument("--prefix_tokens", type=int, default=8)
    p.add_argument("--save_out", type=str, default=None)
    args = p.parse_args()

    main_eval(args.test_pt, args.ckpt_dir, args.prefix_tokens, args.save_out)
