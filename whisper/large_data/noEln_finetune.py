# save as: train_noeln_lora.py
import os, json, torch
from dataclasses import dataclass
from typing import List
from torch.utils.data import Dataset
from transformers import (AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from tqdm import tqdm


MODEL_NAME = "meta-llama/Llama-2-7b-hf"
MAX_INPUT_TOKENS = 1024
LR = 2e-4
EPOCHS = 3
OUTPUT_DIR = "./llama_noeln_lora"
TARGET_MODULES = ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]

INSTR_HEADER = (
    "You are a transcription error correction assistant and linguistics expert, "
    "specializing in improving transcriptions produced by Automatic Speech Recognition (ASR) systems for noisy speeches. "
    "The levels of noise may vary widely. "
    "Your task is to perform error correction based on the words in top 5 hypotheses generated by the ASR system"
    "You can also correct the sentences yourself based on your understanding of their meaning and knowing the correct spelling of the words, but the meaning of the sentences should not be changed. "
    "Also, do not use synonyms, but only correct the spelling of the words.\n\n"
    "Analyze the linguistic context and provide the corrected ASR hypothesis directly without any explanations or additional commentary.\n\n"
)

def build_prompt(hypotheses: List[str]) -> str:
    s = INSTR_HEADER + "Hypotheses:\n"
    for i, h in enumerate(hypotheses, 1):
        s += f"<hypothesis{i}>{h}</hypothesis{i}>\n"
    s += "=>Correct Transcription:"
    return s



class ASRJsonDataset(Dataset):
    def __init__(self, path: str, tokenizer: AutoTokenizer):
        super().__init__()
        self.tokenizer = tokenizer
        with open(path, "r", encoding="utf-8") as f:
            self.records = json.load(f)
        assert isinstance(self.records, list) and len(self.records) > 0, "Empty or invalid dataset!"

    def __len__(self):
        return len(self.records)

    def __getitem__(self, idx):
        r = self.records[idx]
        hyps: List[str] = r["input"]
        label: str = r["output"]

        prompt = build_prompt(hyps)
        prompt_ids = self.tokenizer(prompt, add_special_tokens=False)["input_ids"]
        target_ids = self.tokenizer(label + self.tokenizer.eos_token, add_special_tokens=False)["input_ids"]

        max_prompt = min(len(prompt_ids), MAX_INPUT_TOKENS - len(target_ids))
        prompt_ids = prompt_ids[:max_prompt]
        if len(prompt_ids) + len(target_ids) > MAX_INPUT_TOKENS:
            target_ids = target_ids[:MAX_INPUT_TOKENS - len(prompt_ids)]

        input_ids = prompt_ids + target_ids
        attention_mask = [1] * len(input_ids)
        labels = [-100] * len(prompt_ids) + target_ids

        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.long),
            "labels": torch.tensor(labels, dtype=torch.long),
        }


@dataclass
class DataCollatorPad:
    tokenizer: AutoTokenizer

    def __call__(self, batch):
        max_len = max(len(x["input_ids"]) for x in batch)

        def pad(seq, pad_val):
            out = []
            for x in batch:
                t = x[seq]
                pad_amt = max_len - t.shape[0]
                if pad_amt > 0:
                    t = torch.cat([t, torch.full((pad_amt,), pad_val, dtype=t.dtype)])
                out.append(t)
            return torch.stack(out, dim=0)

        input_ids   = pad("input_ids", pad_val=self.tokenizer.pad_token_id or self.tokenizer.eos_token_id)
        attention   = pad("attention_mask", pad_val=0)
        labels      = pad("labels", pad_val=-100)

        return {
            "input_ids": input_ids,
            "attention_mask": attention,
            "labels": labels
        }


def load_base_model_and_tokenizer():
    tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
    tok.pad_token = tok.eos_token

    base = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        load_in_4bit=True,
        torch_dtype=torch.float16,
        device_map="auto",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
    )
    return base, tok

def wrap_with_lora(model):
    model = prepare_model_for_kbit_training(model)
    lora = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=TARGET_MODULES,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )
    model = get_peft_model(model, lora)
    return model


def main_train(train_json: str):
    base, tok = load_base_model_and_tokenizer()
    base = wrap_with_lora(base)

    train_ds = ASRJsonDataset(train_json, tok)
    collator = DataCollatorPad(tokenizer=tok)

    args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=EPOCHS,
        learning_rate=LR,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=16,
        fp16=True,
        bf16=False,
        lr_scheduler_type="cosine",
        warmup_steps=200,
        weight_decay=0.01,
        gradient_checkpointing=True,
        logging_steps=100,
        save_steps=1000,
        save_total_limit=2,
        report_to="none"
    )

    trainer = Trainer(
        model=base,
        args=args,
        data_collator=collator,
        tokenizer=tok,
        train_dataset=train_ds
    )

    trainer.train()

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    base.save_pretrained(OUTPUT_DIR)
    tok.save_pretrained(OUTPUT_DIR)
    print("Saved to", OUTPUT_DIR)


if __name__ == "__main__":
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument("--train_json", required=True)
    args = p.parse_args()
    main_train(args.train_json)
