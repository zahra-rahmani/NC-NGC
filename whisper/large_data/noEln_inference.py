import os, json, torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from tqdm import tqdm
import editdistance

MODEL_DIR = "./llama_noeln_lora"   # fine-tuned model folder
MAX_INPUT_TOKENS = 1024

# --------------------------
# Prompt builder (same as train)
# --------------------------
INSTR_HEADER = (
    "You are a transcription error correction assistant and linguistics expert, "
    "specializing in improving transcriptions produced by Automatic Speech Recognition (ASR) systems for noisy speeches. "
    "The levels of noise may vary widely. "
    "Your task is to perform error correction based on the words in top 5 hypotheses generated by the ASR system"
    "You can also correct the sentences yourself based on your understanding of their meaning and knowing the correct spelling of the words, but the meaning of the sentences should not be changed. "
    "Also, do not use synonyms, but only correct the spelling of the words.\n\n"
    "Analyze the linguistic context and provide the corrected ASR hypothesis directly without any explanations or additional commentary.\n\n"
)

def build_prompt(hypotheses):
    s = INSTR_HEADER + "Hypotheses:\n"
    for i, h in enumerate(hypotheses, 1):
        s += f"<hypothesis{i}>{h}</hypothesis{i}>\n"
    s += "=>Correct Transcription:"
    return s


# --------------------------
# Load Model
# --------------------------
def load_model():
    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=False)
    tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_DIR,
        device_map="auto",
        torch_dtype=torch.float16,
    )

    # attach LoRA adapters if saved separately
    if os.path.exists(os.path.join(MODEL_DIR, "adapter_config.json")):
        model = PeftModel.from_pretrained(model, MODEL_DIR)

    model.eval()
    return model, tokenizer


# --------------------------
# WER
# --------------------------
def wer(ref, hyp):
    r, h = ref.split(), hyp.split()
    return editdistance.eval(r, h) / max(1, len(r))


# --------------------------
# Evaluation
# --------------------------
def evaluate(test_json, output_file="eval_outputs.jsonl"):
    model, tok = load_model()

    with open(test_json, "r", encoding="utf-8") as f:
        test_records = json.load(f)

    outputs, wers = [], []
    for r in tqdm(test_records):
        hyps = r["input"]
        ref = r["output"]

        prompt = build_prompt(hyps)
        input_ids = tok(prompt, return_tensors="pt", truncation=True, max_length=MAX_INPUT_TOKENS).to(model.device)

        with torch.no_grad():
            gen_ids = model.generate(
                **input_ids,
                max_new_tokens=128,
                temperature=0.7,
                do_sample=False
            )

        gen_text = tok.decode(gen_ids[0], skip_special_tokens=True)
        # Extract only after "=>Correct Transcription:"
        if "=>Correct Transcription:" in gen_text:
            gen_text = gen_text.split("=>Correct Transcription:")[-1].strip()

        w = wer(ref, gen_text)
        wers.append(w)

        outputs.append({
            "input": hyps,
            "reference": ref,
            "prediction": gen_text,
            "wer": w
        })

    # save results
    with open(output_file, "w", encoding="utf-8") as f:
        for o in outputs:
            f.write(json.dumps(o, ensure_ascii=False) + "\n")

    avg_wer = sum(wers) / len(wers)
    print(f"Saved predictions to {output_file}")
    print(f"Average WER: {avg_wer:.4f}")
    return outputs, avg_wer


if __name__ == "__main__":
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument("--test_json", required=True)
    p.add_argument("--out", default="noEln_results.jsonl")
    args = p.parse_args()
    evaluate(args.test_json, args.out)
